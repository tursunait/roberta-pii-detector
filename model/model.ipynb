{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b0e9f33",
   "metadata": {},
   "source": [
    "# Step 1 - Double Check proper enviroment setup\n",
    "\n",
    "* write this in terminal \"conda activate pii\n",
    "python -m ipykernel install --user --name=pii --display-name \"Python (pii)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9c9f2ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/pii/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cef47ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets version: 4.4.1\n",
      "transformers version: 4.57.1\n",
      "torch version: 2.9.1\n",
      "torch version: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "# Quick check that key packages are available\n",
    "import datasets\n",
    "import transformers\n",
    "import torch\n",
    "import pandas\n",
    "\n",
    "print(f\"datasets version: {datasets.__version__}\")\n",
    "print(f\"transformers version: {transformers.__version__}\")\n",
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"torch version: {pandas.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "81fe1997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the repository:\n",
      "  .gitattributes\n",
      "  README.md\n",
      "  test/data-00000-of-00001.arrow\n",
      "  test/dataset_info.json\n",
      "  test/state.json\n",
      "  train/.gitattributes\n",
      "  train/README.md\n",
      "  train/data-00000-of-00002.arrow\n",
      "  train/data-00001-of-00002.arrow\n",
      "  train/dataset_info.json\n",
      "  train/state.json\n",
      "  val/data-00000-of-00001.arrow\n",
      "  val/dataset_info.json\n",
      "  val/state.json\n"
     ]
    }
   ],
   "source": [
    "from datasets import get_dataset_config_names, get_dataset_split_names\n",
    "from huggingface_hub import list_repo_files\n",
    "\n",
    "# See what files are actually in the repository\n",
    "files = list_repo_files(\"tursunait/deberta-pii-synth\", repo_type=\"dataset\")\n",
    "print(\"Files in the repository:\")\n",
    "for f in files:\n",
    "    print(f\"  {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8d86cc",
   "metadata": {},
   "source": [
    "# Step 2: Load and Analyse Dataset\n",
    "\n",
    "* Issue: This confirms the issue: train uses Arrow format, but validation and test use JSON format! This is a misconfiguration in the dataset on Hugging Face itself.The dataset IS actually all in Arrow format, but there's a configuration issue preventing it from loading automatically. So use the exact code below not the one given by Tursunai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c885fe86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 96000\n",
      "Val size: 12000\n",
      "Test size: 12000\n",
      "\n",
      "First example:\n",
      "{'text': 'Jack Phillips fr7m Hancock-Melendez uqes card 4843127370283685 on 1994-08-27.', 'spans': [{'end': 13, 'label': 'PERSON', 'start': 0}, {'end': 35, 'label': 'ORG', 'start': 19}, {'end': 62, 'label': 'CREDIT_CARD', 'start': 46}, {'end': 76, 'label': 'DATE', 'start': 66}], 'input_ids': [1, 20907, 7431, 6664, 406, 119, 19632, 12, 20201, 22192, 1717, 1343, 293, 1886, 2929, 3897, 1092, 5352, 3083, 2517, 3367, 4531, 15, 8148, 12, 3669, 12, 2518, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 17, 19, 0, 0, 0, 21, 22, 22, 23, 0, 0, 0, 0, 13, 14, 14, 14, 14, 14, 14, 15, 0, 29, 30, 30, 30, 31, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], 'tokens': ['[CLS]', 'Jack', 'ƒ†Phillips', 'ƒ†fr', '7', 'm', 'ƒ†Hancock', '-', 'Mel', 'endez', 'ƒ†u', 'q', 'es', 'ƒ†card', 'ƒ†48', '43', '12', '73', '70', '28', '36', '85', 'ƒ†on', 'ƒ†1994', '-', '08', '-', '27', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load by explicitly pointing to the arrow files\n",
    "ds = load_dataset(\n",
    "    \"arrow\",\n",
    "    data_files={\n",
    "        \"train\": \"hf://datasets/tursunait/deberta-pii-synth/train/data-*.arrow\",\n",
    "        \"validation\": \"hf://datasets/tursunait/deberta-pii-synth/val/data-*.arrow\",\n",
    "        \"test\": \"hf://datasets/tursunait/deberta-pii-synth/test/data-*.arrow\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Dataset already split\n",
    "train = ds[\"train\"]\n",
    "val = ds[\"validation\"]\n",
    "test = ds[\"test\"]\n",
    "\n",
    "print(\"Train size:\", len(train))\n",
    "print(\"Val size:\", len(val))\n",
    "print(\"Test size:\", len(test))\n",
    "print(\"\\nFirst example:\")\n",
    "print(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5482910c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing FULL training dataset...\n",
      "\n",
      "üìä Sequence Length Statistics:\n",
      "Total examples: 96,000\n",
      "Average length: 256.0 tokens\n",
      "Median length: 256.0 tokens\n",
      "Min length: 256 tokens\n",
      "Max length: 256 tokens\n",
      "Std deviation: 0.0\n",
      "\n",
      "üìà Length Distribution:\n",
      "Sequences < 50 tokens: 0 (0.0%)\n",
      "Sequences 50-100 tokens: 0 (0.0%)\n",
      "Sequences 100-200 tokens: 0 (0.0%)\n",
      "Sequences 200-512 tokens: 96,000 (100.0%)\n",
      "Sequences 512+ tokens: 0 (0.0%)\n",
      "\n",
      "‚è±Ô∏è Time Estimation:\n",
      "With batch_size=8: ~12,000 steps per epoch\n",
      "Estimated time at ~0.5 sec/step: 1.7 hours per epoch\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"üîç Analyzing FULL training dataset...\")\n",
    "\n",
    "# Check ALL training examples\n",
    "lengths = [len(example['input_ids']) for example in train]\n",
    "\n",
    "print(f\"\\nüìä Sequence Length Statistics:\")\n",
    "print(f\"Total examples: {len(train):,}\")\n",
    "print(f\"Average length: {np.mean(lengths):.1f} tokens\")\n",
    "print(f\"Median length: {np.median(lengths):.1f} tokens\")\n",
    "print(f\"Min length: {np.min(lengths)} tokens\")\n",
    "print(f\"Max length: {np.max(lengths)} tokens\")\n",
    "print(f\"Std deviation: {np.std(lengths):.1f}\")\n",
    "\n",
    "# Distribution analysis\n",
    "print(f\"\\nüìà Length Distribution:\")\n",
    "print(f\"Sequences < 50 tokens: {sum(1 for l in lengths if l < 50):,} ({sum(1 for l in lengths if l < 50)/len(lengths)*100:.1f}%)\")\n",
    "print(f\"Sequences 50-100 tokens: {sum(1 for l in lengths if 50 <= l < 100):,} ({sum(1 for l in lengths if 50 <= l < 100)/len(lengths)*100:.1f}%)\")\n",
    "print(f\"Sequences 100-200 tokens: {sum(1 for l in lengths if 100 <= l < 200):,} ({sum(1 for l in lengths if 100 <= l < 200)/len(lengths)*100:.1f}%)\")\n",
    "print(f\"Sequences 200-512 tokens: {sum(1 for l in lengths if 200 <= l < 512):,} ({sum(1 for l in lengths if 200 <= l < 512)/len(lengths)*100:.1f}%)\")\n",
    "print(f\"Sequences 512+ tokens: {sum(1 for l in lengths if l >= 512):,} ({sum(1 for l in lengths if l >= 512)/len(lengths)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n‚è±Ô∏è Time Estimation:\")\n",
    "print(f\"With batch_size=8: ~{len(train)//8:,} steps per epoch\")\n",
    "print(f\"Estimated time at ~0.5 sec/step: {(len(train)//8 * 0.5)/3600:.1f} hours per epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea48982",
   "metadata": {},
   "source": [
    "# Step 3: Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd5924b",
   "metadata": {},
   "source": [
    "## 3.1 - Load DeBERTa-base\n",
    "- DeBERTa is a pre-trained language model (like a smart AI that already understands language). You need to:\n",
    "\n",
    "    - Load the base model called \"microsoft/deberta-base\"\n",
    "    - Load it specifically for token classification (labeling each word/token)\\\n",
    "    - The model needs to know how many labels exist (like PERSON, ORG, EMAIL, etc.)\n",
    "    - Also load the tokenizer (converts text to numbers the model understands)\n",
    "\n",
    "- Think of it like: You're taking a smart student (DeBERTa) who already knows English, and now you're going to teach them to specifically identify PII in text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb970c5",
   "metadata": {},
   "source": [
    "## 3.2 - Baseline Model: Fixed Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358c4c63",
   "metadata": {},
   "source": [
    "### A. Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a07352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading roberta-base...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fba8ebe41094dc090c1b1c295857952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6fc533b95f4e8dbde68bcb40b2e87f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03fb2f7fdc464c4aae966aaf6d75bdb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee00cb177f040c0b895c17732d9e63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3fc1790a35415aa2c2c22ce3150c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e570d7f701b24f9cae8cb5d9ac4a3d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/_5/g20j99hd5j91r849_d_zgb040000gn/T/ipykernel_54221/1821411500.py:67: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded with 33 labels\n",
      "üìä Creating small subsets for fast tuning...\n",
      "Train subset: 3000 examples\n",
      "Val subset: 500 examples\n",
      "\n",
      "üöÄ Starting training with roberta-base...\n",
      "‚è±Ô∏è  This should work without overflow errors!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [188/188 10:40, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training complete!\n",
      "\n",
      "üìà Evaluating...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: {'eval_loss': 0.02069513127207756, 'eval_runtime': 25.2524, 'eval_samples_per_second': 19.8, 'eval_steps_per_second': 2.495, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Import label configuration\n",
    "from pii_synth.config_and_labels import LABEL2ID, ID2LABEL\n",
    "\n",
    "# Step 1: Call the model - RoBERTa (Most stable and popular for NER)\n",
    "model_name = \"roberta-base\"\n",
    "\n",
    "num_labels = len(ID2LABEL)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    id2label=ID2LABEL,\n",
    "    label2id=LABEL2ID\n",
    ")\n",
    "\n",
    "# Step 2: Create SMALL subsets for fast hyperparameter tuning - IF WE USE FULL MODEL IT BREAKS\n",
    "train_subset = train.select(range(3000))\n",
    "val_subset = val.select(range(500))\n",
    "\n",
    "print(f\"Train subset: {len(train_subset)} examples\")\n",
    "print(f\"Val subset: {len(val_subset)} examples\")\n",
    "\n",
    "# Step 3: Training configuration - STABLE SETTINGS -- \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_baseline_model\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-5,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    "    max_grad_norm=1.0,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    dataloader_num_workers=0,\n",
    ")\n",
    "\n",
    "# Step 4: Data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "# Step 5: Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_subset,\n",
    "    eval_dataset=val_subset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Step 6: Train\n",
    "print(f\"\\n Starting training with {model_name}...\")\n",
    "trainer.train()\n",
    "print(\"‚úÖ Training complete!\")\n",
    "\n",
    "# Step 7: Evaluate\n",
    "print(\"\\nüìà Evaluating...\")\n",
    "results = trainer.evaluate()\n",
    "print(f\"Validation results: {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de159fc0",
   "metadata": {},
   "source": [
    "### B. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "db7d593d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/pii/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prediction step done.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# (Optional) use smaller test subset\n",
    "test_eval = test.select(range(300))   # or test for full set\n",
    "\n",
    "# 1) Run predictions on the test set\n",
    "test_predictions = trainer.predict(test_eval)\n",
    "\n",
    "logits = test_predictions.predictions   # [batch, seq_len, num_labels]\n",
    "labels = test_predictions.label_ids     # [batch, seq_len]\n",
    "\n",
    "# 2) Convert logits ‚Üí predicted label IDs\n",
    "pred_ids = np.argmax(logits, axis=-1)\n",
    "\n",
    "# 3) Remove padding tokens (label == -100)\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "for p, t in zip(pred_ids, labels):\n",
    "    mask = t != -100\n",
    "    all_preds.append(p[mask])\n",
    "    all_labels.append(t[mask])\n",
    "\n",
    "# Flatten for simple metrics\n",
    "all_preds_flat = np.concatenate(all_preds)\n",
    "all_labels_flat = np.concatenate(all_labels)\n",
    "\n",
    "print(\"Test prediction step done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77a268e",
   "metadata": {},
   "source": [
    "### C. Performance Evaluation - METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d3cc18f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token-level Accuracy: 0.9978969505783386\n",
      "\n",
      "Precision: 0.9803571428571428\n",
      "Recall: 0.987410071942446\n",
      "F1 Score: 0.9838709677419355\n",
      "\n",
      "Detailed classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     ADDRESS       0.98      1.00      0.99        44\n",
      " CREDIT_CARD       0.98      1.00      0.99        61\n",
      "        DATE       1.00      1.00      1.00       107\n",
      "       EMAIL       0.97      0.97      0.97        61\n",
      "         ORG       0.99      0.99      0.99        96\n",
      "      PERSON       0.96      0.97      0.96       112\n",
      "       PHONE       1.00      1.00      1.00        57\n",
      "         SSN       0.94      0.94      0.94        18\n",
      "\n",
      "   micro avg       0.98      0.99      0.98       556\n",
      "   macro avg       0.98      0.98      0.98       556\n",
      "weighted avg       0.98      0.99      0.98       556\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/pii/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-DATE seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/miniconda3/envs/pii/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-PHONE seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/miniconda3/envs/pii/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-PERSON seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/miniconda3/envs/pii/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-ORG seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/miniconda3/envs/pii/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-EMAIL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/miniconda3/envs/pii/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-ADDRESS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/miniconda3/envs/pii/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-CREDIT_CARD seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/miniconda3/envs/pii/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-SSN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# 1) Token-level accuracy (mostly meaningless for NER but fine to report)\n",
    "accuracy = accuracy_score(all_labels_flat, all_preds_flat)\n",
    "print(\"Token-level Accuracy:\", accuracy)\n",
    "\n",
    "# 2) Convert label IDs ‚Üí text labels for seqeval\n",
    "id2label = ID2LABEL   # your mapping from earlier\n",
    "\n",
    "grouped_preds = []\n",
    "grouped_labels = []\n",
    "\n",
    "for p_seq, t_seq in zip(all_preds, all_labels):\n",
    "    grouped_preds.append([id2label[int(i)] for i in p_seq])\n",
    "    grouped_labels.append([id2label[int(i)] for i in t_seq])\n",
    "\n",
    "# 3) Print real NER metrics\n",
    "print(\"\\nPrecision:\", precision_score(grouped_labels, grouped_preds))\n",
    "print(\"Recall:\", recall_score(grouped_labels, grouped_preds))\n",
    "print(\"F1 Score:\", f1_score(grouped_labels, grouped_preds))\n",
    "\n",
    "print(\"\\nDetailed classification report:\")\n",
    "print(classification_report(grouped_labels, grouped_preds))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
