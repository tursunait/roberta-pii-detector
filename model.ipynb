{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b0e9f33",
   "metadata": {},
   "source": [
    "# Step 1 - Double Check proper enviroment setup\n",
    "\n",
    "* write this in terminal \"conda activate pii\n",
    "python -m ipykernel install --user --name=pii --display-name \"Python (pii)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c9f2ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/pii/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cef47ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets version: 4.4.1\n",
      "transformers version: 4.57.1\n",
      "torch version: 2.9.1\n",
      "torch version: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "# Quick check that key packages are available\n",
    "import datasets\n",
    "import transformers\n",
    "import torch\n",
    "import pandas\n",
    "\n",
    "print(f\"datasets version: {datasets.__version__}\")\n",
    "print(f\"transformers version: {transformers.__version__}\")\n",
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"torch version: {pandas.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81fe1997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import get_dataset_config_names, get_dataset_split_names\n",
    "# from huggingface_hub import list_repo_files\n",
    "\n",
    "# # See what files are actually in the repository\n",
    "# files = list_repo_files(\"tursunait/deberta-pii-synth\", repo_type=\"dataset\")\n",
    "# print(\"Files in the repository:\")\n",
    "# for f in files:\n",
    "#     print(f\"  {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0f1fdf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTITY_TYPES: ['EMAIL', 'PHONE', 'SSN', 'CREDIT_CARD', 'PERSON', 'ORG', 'ADDRESS', 'DATE', 'AGE']\n",
      "Number of entity types: 9\n",
      "Total labels: 37\n",
      "Expected: 1 + 9 √ó 4 = 37\n",
      "‚úÖ Config is correct!\n",
      "\n",
      "num_labels for model: 37\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# ============================================================================\n",
    "# CRITICAL: Force reimport of config to get AGE\n",
    "# ============================================================================\n",
    "\n",
    "# Remove cached import\n",
    "if 'pii_synth.config_and_labels' in sys.modules:\n",
    "    del sys.modules['pii_synth.config_and_labels']\n",
    "\n",
    "# Fresh import\n",
    "from pii_synth.config_and_labels import LABEL_LIST, ENTITY_TYPES\n",
    "\n",
    "# ============================================================================\n",
    "# Verify labels\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"ENTITY_TYPES: {ENTITY_TYPES}\")\n",
    "print(f\"Number of entity types: {len(ENTITY_TYPES)}\")\n",
    "print(f\"Total labels: {len(LABEL_LIST)}\")\n",
    "print(f\"Expected: 1 + {len(ENTITY_TYPES)} √ó 4 = {1 + len(ENTITY_TYPES) * 4}\")\n",
    "\n",
    "if len(ENTITY_TYPES) == 9 and len(LABEL_LIST) == 37:\n",
    "    print(\"‚úÖ Config is correct!\")\n",
    "else:\n",
    "    print(\"‚ùå Config mismatch!\")\n",
    "    \n",
    "# ============================================================================\n",
    "# Create label mappings\n",
    "# ============================================================================\n",
    "\n",
    "ID2LABEL = {i: label for i, label in enumerate(LABEL_LIST)}\n",
    "LABEL2ID = {label: i for i, label in enumerate(LABEL_LIST)}\n",
    "\n",
    "num_labels = len(LABEL_LIST)\n",
    "\n",
    "print(f\"\\nnum_labels for model: {num_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8d86cc",
   "metadata": {},
   "source": [
    "# Step 2: Load and Analyse Dataset\n",
    "\n",
    "* Issue: This confirms the issue: train uses Arrow format, but validation and test use JSON format! This is a misconfiguration in the dataset on Hugging Face itself.The dataset IS actually all in Arrow format, but there's a configuration issue preventing it from loading automatically. So use the exact code below not the one given by Tursunai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c885fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# # Load by explicitly pointing to the arrow files\n",
    "# ds = load_dataset(\n",
    "#     \"arrow\",\n",
    "#     data_files={\n",
    "#         \"train\": \"hf://datasets/tursunait/deberta-pii-synth/train/data-*.arrow\",\n",
    "#         \"validation\": \"hf://datasets/tursunait/deberta-pii-synth/val/data-*.arrow\",\n",
    "#         \"test\": \"hf://datasets/tursunait/deberta-pii-synth/test/data-*.arrow\"\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# NEW CODE (loads from local):\n",
    "\n",
    "# # Dataset already split\n",
    "# train = ds[\"train\"]\n",
    "# val = ds[\"validation\"]\n",
    "# test = ds[\"test\"]\n",
    "\n",
    "# print(\"Train size:\", len(train))\n",
    "# print(\"Val size:\", len(val))\n",
    "# print(\"Test size:\", len(test))\n",
    "# print(\"\\nFirst example:\")\n",
    "# print(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58a5dba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 96000\n",
      "Val size: 12000\n",
      "Test size: 12000\n",
      "\n",
      "First example:\n",
      "{'text': \"DON'T SHACE buX uZrE's 1970rodney.lewis'S coMatctD sarahperez@aol.com /g2118x174 / ssn 0651734596\", 'spans': [{'end': 39, 'label': 'PERSON', 'start': 23}, {'end': 69, 'label': 'EMAIL', 'start': 51}, {'end': 80, 'label': 'PHONE', 'start': 72}, {'end': 97, 'label': 'SSN', 'start': 87}], 'input_ids': [1, 42737, 108, 565, 4584, 15949, 10306, 1000, 1717, 1301, 338, 717, 18, 6200, 10774, 2596, 4, 459, 605, 354, 108, 104, 1029, 30121, 3894, 495, 579, 36000, 2379, 13208, 1039, 102, 1168, 4, 175, 1589, 571, 176, 21369, 1178, 29221, 1589, 579, 22617, 321, 3506, 30664, 1898, 5607, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 18, 18, 18, 18, 18, 19, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 3, 0, 0, 5, 6, 6, 7, 0, 0, 0, 9, 10, 10, 10, 11, -100], 'tokens': ['[CLS]', 'DON', \"'\", 'T', 'ƒ†SH', 'ACE', 'ƒ†bu', 'X', 'ƒ†u', 'Z', 'r', 'E', \"'s\", 'ƒ†1970', 'rod', 'ney', '.', 'le', 'w', 'is', \"'\", 'S', 'ƒ†co', 'Mat', 'ct', 'D', 'ƒ†s', 'arah', 'pe', 'rez', '@', 'a', 'ol', '.', 'com', 'ƒ†/', 'g', '2', '118', 'x', '174', 'ƒ†/', 'ƒ†s', 'sn', 'ƒ†0', '65', '173', '45', '96', '[SEP]']}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "train = load_from_disk(\"data/processed/train\")\n",
    "val = load_from_disk(\"data/processed/val\")\n",
    "test = load_from_disk(\"data/processed/test\")\n",
    "\n",
    "print(\"Train size:\", len(train))\n",
    "print(\"Val size:\", len(val))\n",
    "print(\"Test size:\", len(test))\n",
    "print(\"\\nFirst example:\")\n",
    "print(train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "789a74a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample labels: [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 18, 18, 18, 18, 18, 19, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 3, 0, 0, 5, 6, 6, 7, 0, 0, 0, 9, 10, 10, 10, 11, -100]\n",
      "\n",
      "Max label ID found in first 100 examples: 36\n",
      "This means we need at least 37 labels\n",
      "\n",
      "ENTITY_TYPES: ['EMAIL', 'PHONE', 'SSN', 'CREDIT_CARD', 'PERSON', 'ORG', 'ADDRESS', 'DATE', 'AGE']\n",
      "Number of entity types: 9\n",
      "LABEL_LIST length: 37\n",
      "\n",
      "Expected labels: 1 (O) + 9 entities √ó 4 (BILOU) = 37\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load data\n",
    "train = load_from_disk(\"data/processed/train\")\n",
    "\n",
    "# Check first example\n",
    "sample = train[0]\n",
    "print(f\"Sample labels: {sample['labels']}\")\n",
    "\n",
    "# Find max label\n",
    "all_max_labels = []\n",
    "for i in range(min(100, len(train))):\n",
    "    labels = [l for l in train[i]['labels'] if l != -100]\n",
    "    if labels:\n",
    "        all_max_labels.append(max(labels))\n",
    "\n",
    "print(f\"\\nMax label ID found in first 100 examples: {max(all_max_labels)}\")\n",
    "print(f\"This means we need at least {max(all_max_labels) + 1} labels\")\n",
    "\n",
    "# Check what's in config\n",
    "from pii_synth.config_and_labels import LABEL_LIST, ENTITY_TYPES\n",
    "\n",
    "print(f\"\\nENTITY_TYPES: {ENTITY_TYPES}\")\n",
    "print(f\"Number of entity types: {len(ENTITY_TYPES)}\")\n",
    "print(f\"LABEL_LIST length: {len(LABEL_LIST)}\")\n",
    "print(f\"\\nExpected labels: 1 (O) + {len(ENTITY_TYPES)} entities √ó 4 (BILOU) = {1 + len(ENTITY_TYPES) * 4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5482910c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing FULL training dataset...\n",
      "\n",
      "üìä Sequence Length Statistics:\n",
      "Total examples: 96,000\n",
      "Average length: 46.0 tokens\n",
      "Median length: 28.0 tokens\n",
      "Min length: 5 tokens\n",
      "Max length: 294 tokens\n",
      "Std deviation: 48.4\n",
      "\n",
      "üìà Length Distribution:\n",
      "Sequences < 50 tokens: 74,534 (77.6%)\n",
      "Sequences 50-100 tokens: 9,231 (9.6%)\n",
      "Sequences 100-200 tokens: 10,113 (10.5%)\n",
      "Sequences 200-512 tokens: 2,122 (2.2%)\n",
      "Sequences 512+ tokens: 0 (0.0%)\n",
      "\n",
      "‚è±Ô∏è Time Estimation:\n",
      "With batch_size=8: ~12,000 steps per epoch\n",
      "Estimated time at ~0.5 sec/step: 1.7 hours per epoch\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"üîç Analyzing FULL training dataset...\")\n",
    "\n",
    "# Check ALL training examples\n",
    "lengths = [len(example['input_ids']) for example in train]\n",
    "\n",
    "print(f\"\\nüìä Sequence Length Statistics:\")\n",
    "print(f\"Total examples: {len(train):,}\")\n",
    "print(f\"Average length: {np.mean(lengths):.1f} tokens\")\n",
    "print(f\"Median length: {np.median(lengths):.1f} tokens\")\n",
    "print(f\"Min length: {np.min(lengths)} tokens\")\n",
    "print(f\"Max length: {np.max(lengths)} tokens\")\n",
    "print(f\"Std deviation: {np.std(lengths):.1f}\")\n",
    "\n",
    "# Distribution analysis\n",
    "print(f\"\\nüìà Length Distribution:\")\n",
    "print(f\"Sequences < 50 tokens: {sum(1 for l in lengths if l < 50):,} ({sum(1 for l in lengths if l < 50)/len(lengths)*100:.1f}%)\")\n",
    "print(f\"Sequences 50-100 tokens: {sum(1 for l in lengths if 50 <= l < 100):,} ({sum(1 for l in lengths if 50 <= l < 100)/len(lengths)*100:.1f}%)\")\n",
    "print(f\"Sequences 100-200 tokens: {sum(1 for l in lengths if 100 <= l < 200):,} ({sum(1 for l in lengths if 100 <= l < 200)/len(lengths)*100:.1f}%)\")\n",
    "print(f\"Sequences 200-512 tokens: {sum(1 for l in lengths if 200 <= l < 512):,} ({sum(1 for l in lengths if 200 <= l < 512)/len(lengths)*100:.1f}%)\")\n",
    "print(f\"Sequences 512+ tokens: {sum(1 for l in lengths if l >= 512):,} ({sum(1 for l in lengths if l >= 512)/len(lengths)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n‚è±Ô∏è Time Estimation:\")\n",
    "print(f\"With batch_size=8: ~{len(train)//8:,} steps per epoch\")\n",
    "print(f\"Estimated time at ~0.5 sec/step: {(len(train)//8 * 0.5)/3600:.1f} hours per epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea48982",
   "metadata": {},
   "source": [
    "# Step 3: Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd5924b",
   "metadata": {},
   "source": [
    "## 3.1 - Load DeBERTa-base\n",
    "- DeBERTa is a pre-trained language model (like a smart AI that already understands language). You need to:\n",
    "\n",
    "    - Load the base model called \"microsoft/deberta-base\"\n",
    "    - Load it specifically for token classification (labeling each word/token)\\\n",
    "    - The model needs to know how many labels exist (like PERSON, ORG, EMAIL, etc.)\n",
    "    - Also load the tokenizer (converts text to numbers the model understands)\n",
    "\n",
    "- Think of it like: You're taking a smart student (DeBERTa) who already knows English, and now you're going to teach them to specifically identify PII in text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb970c5",
   "metadata": {},
   "source": [
    "## 3.2 - Baseline Model: Fixed Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358c4c63",
   "metadata": {},
   "source": [
    "### A. Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7437f4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Model created with 37 labels\n",
      "Model config num_labels: 37\n",
      "Train subset: 6000 examples\n",
      "Val subset: 600 examples\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "import os\n",
    "\n",
    "\n",
    "# Step 1: Call the model - RoBERTa (Most stable and popular for NER)\n",
    "# ============================================================================\n",
    "# Initialize model\n",
    "# ============================================================================\n",
    "\n",
    "model_name = \"roberta-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Force CPU\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Create model with correct number of labels\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,  # Should be 37\n",
    "    id2label=ID2LABEL,\n",
    "    label2id=LABEL2ID,\n",
    "    ignore_mismatched_sizes=True  # Important!\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\n‚úÖ Model created with {num_labels} labels\")\n",
    "print(f\"Model config num_labels: {model.config.num_labels}\")\n",
    "\n",
    "\n",
    "# Step 2: Create SMALL subsets for fast hyperparameter tuning - IF WE USE FULL MODEL IT BREAKS\n",
    "train_subset = train.select(range(6000))\n",
    "val_subset = val.select(range(600))\n",
    "\n",
    "print(f\"Train subset: {len(train_subset)} examples\")\n",
    "print(f\"Val subset: {len(val_subset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1a0055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Training configuration - STABLE SETTINGS -- \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_baseline_model\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-5,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    "    max_grad_norm=1.0,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    dataloader_num_workers=0,\n",
    "    use_cpu=True # ‚Üê Add this to run without cpu issues\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f47ce85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data collator created\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Data Collator\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Data collator created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91f280a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_5/g20j99hd5j91r849_d_zgb040000gn/T/ipykernel_11255/1870159824.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(  # Changed this line!\n"
     ]
    }
   ],
   "source": [
    "# Use regular Trainer\n",
    "trainer = Trainer(  # Changed this line!\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_subset,\n",
    "    eval_dataset=val_subset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c520415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/pii/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Starting training with roberta-base...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='376' max='376' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [376/376 23:47, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.217800</td>\n",
       "      <td>0.167246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Train\n",
    "print(f\"\\n Starting training with {model_name}...\")\n",
    "trainer.train()\n",
    "print(\"‚úÖ Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "401a8761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Evaluating...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: {'eval_loss': 0.12513229250907898, 'eval_runtime': 23.2881, 'eval_samples_per_second': 25.764, 'eval_steps_per_second': 3.221, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Evaluate\n",
    "print(\"\\nüìà Evaluating...\")\n",
    "results = trainer.evaluate()\n",
    "print(f\"Validation results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd76863e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving your trained model...\n",
      "‚úÖ Model saved to: ./trained_model\n",
      "üìÅ Model directory contains: ['model.safetensors', 'tokenizer_config.json', 'special_tokens_map.json', 'config.json', 'tokenizer.json', 'merges.txt', 'training_args.bin', 'vocab.json']\n"
     ]
    }
   ],
   "source": [
    "# Save your trained model properly\n",
    "print(\"Saving your trained model...\")\n",
    "\n",
    "# Create a directory for your model\n",
    "model_save_path = \"./trained_model\"\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "# Save the model and tokenizer\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {model_save_path}\")\n",
    "\n",
    "# Verify the files were created\n",
    "model_files = os.listdir(model_save_path)\n",
    "print(f\"üìÅ Model directory contains: {model_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de159fc0",
   "metadata": {},
   "source": [
    "### B. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db7d593d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prediction step done.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# (Optional) use smaller test subset\n",
    "test_eval = test.select(range(600))   # or test for full set\n",
    "\n",
    "# 1) Run predictions on the test set\n",
    "test_predictions = trainer.predict(test_eval)\n",
    "\n",
    "logits = test_predictions.predictions   # [batch, seq_len, num_labels]\n",
    "labels = test_predictions.label_ids     # [batch, seq_len]\n",
    "\n",
    "# 2) Convert logits ‚Üí predicted label IDs\n",
    "pred_ids = np.argmax(logits, axis=-1)\n",
    "\n",
    "# 3) Remove padding tokens (label == -100)\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "for p, t in zip(pred_ids, labels):\n",
    "    mask = t != -100\n",
    "    all_preds.append(p[mask])\n",
    "    all_labels.append(t[mask])\n",
    "\n",
    "# Flatten for simple metrics\n",
    "all_preds_flat = np.concatenate(all_preds)\n",
    "all_labels_flat = np.concatenate(all_labels)\n",
    "\n",
    "print(\"Test prediction step done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77a268e",
   "metadata": {},
   "source": [
    "### C. Performance Evaluation - METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3cc18f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token-level Accuracy: 0.9689050985110543\n",
      "\n",
      "Precision: 0.8337874659400545\n",
      "Recall: 0.865874363327674\n",
      "F1 Score: 0.84952803997779\n",
      "\n",
      "Detailed classification report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/pii/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-PHONE seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/miniconda3/envs/pii/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-CREDIT_CARD seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/miniconda3/envs/pii/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-DATE seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/miniconda3/envs/pii/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-PERSON seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/miniconda3/envs/pii/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-EMAIL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/miniconda3/envs/pii/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-SSN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/miniconda3/envs/pii/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-AGE seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/miniconda3/envs/pii/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-DATE seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/miniconda3/envs/pii/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-ADDRESS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/miniconda3/envs/pii/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L-ORG seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/miniconda3/envs/pii/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-AGE seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/miniconda3/envs/pii/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-PERSON seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/miniconda3/envs/pii/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: U-ADDRESS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     ADDRESS       0.79      0.70      0.74       122\n",
      "         AGE       0.66      0.76      0.71        95\n",
      " CREDIT_CARD       0.87      0.93      0.90        74\n",
      "        DATE       0.93      0.93      0.93       258\n",
      "       EMAIL       0.91      0.93      0.92       267\n",
      "         ORG       0.93      0.94      0.94        87\n",
      "      PERSON       0.81      0.86      0.84       448\n",
      "       PHONE       0.81      0.85      0.83       294\n",
      "         SSN       0.72      0.80      0.76       122\n",
      "\n",
      "   micro avg       0.83      0.87      0.85      1767\n",
      "   macro avg       0.83      0.86      0.84      1767\n",
      "weighted avg       0.84      0.87      0.85      1767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# 1) Token-level accuracy (mostly meaningless for NER but fine to report)\n",
    "accuracy = accuracy_score(all_labels_flat, all_preds_flat)\n",
    "print(\"Token-level Accuracy:\", accuracy)\n",
    "\n",
    "# 2) Convert label IDs ‚Üí text labels for seqeval\n",
    "id2label = ID2LABEL   # your mapping from earlier\n",
    "\n",
    "grouped_preds = []\n",
    "grouped_labels = []\n",
    "\n",
    "for p_seq, t_seq in zip(all_preds, all_labels):\n",
    "    grouped_preds.append([id2label[int(i)] for i in p_seq])\n",
    "    grouped_labels.append([id2label[int(i)] for i in t_seq])\n",
    "\n",
    "# 3) Print real NER metrics\n",
    "print(\"\\nPrecision:\", precision_score(grouped_labels, grouped_preds))\n",
    "print(\"Recall:\", recall_score(grouped_labels, grouped_preds))\n",
    "print(\"F1 Score:\", f1_score(grouped_labels, grouped_preds))\n",
    "\n",
    "print(\"\\nDetailed classification report:\")\n",
    "print(classification_report(grouped_labels, grouped_preds))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
